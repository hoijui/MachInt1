\documentclass[a4paper,headings=small]{scrartcl}
\KOMAoptions{DIV=12}

\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}

% instead of using indents to denote a new paragraph, we add space before it
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt plus 1pt minus 1pt}

\title{Machine Intelligence I - WS2011/2012\\Excercise 9}
\author{Robin Vobruba, Rolf Schroeder, Marcus Grum, Robert Koppisch}
\date{\today}

\pdfinfo{%
  /Title    (Machine Intelligence I - WS2011/2012 - Excercise 9)
  /Author   (Robin Vobruba, Rolf Schroeder, Marcus Grum, Robert K oppisch)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

% Simple picture reference
%   Usage: \image{#1}{#2}{#3}
%     #1: file-name of the image
%     #2: percentual width (decimal)
%     #3: caption/description
%
%   Example:
%     \image{myPicture}{0.8}{My huge house}
%     See fig. \ref{fig:myPicture}.
\newcommand{\image}[3]{
\begin{figure}[htbp]
\centering
\includegraphics[width=#2\textwidth]{#1}
\caption{#3}
\label{fig:#1}
\end{figure}
}



\begin{document}


\maketitle

\section{Implementations}
As software platform we decided to use Octave of the Version 3.4.3, because it is similar to Mathlab of the version R2007b, where the majority of the group members had already experiences in programming with, and it is opensource software.\\
\\
The program itself will be started in commenting in the the functions that are described in the following three subpoints. Each function starts one run of a classification technique with a certain set of parameters. More specific informations can be seen at the specific subpoints.\\
\\
In general, before applying at least one of the three classification techniques, there are some steps done as initialisation. Those are presented in the following:\\
\\
The $DataTrainingP$ is built in using the $generateSamples()$ function. Here, the samples are generated w.r.t. a mixture of Gaussians with centers in XOR-configuration in the first two loops, where p is incremented. In the second two loops, where s is incremented, the two centers are related to the previous generated samples. All those are returned. Hence , $DataTrainingP$ looks like the following line shows:
\begin{align}
DataTrainingP = [samplesC1Y; samplesC1X; samplesC2Y; samplesC2X]
\end{align}
The $DataTrainingC$ is built for handling the plotting steps easier and rearranges the elements of $DataTrainingP$ as you can see in the following line:
\begin{align}
DataTrainingC = 
\left(
%this is a table
\begin{array}{cccc}
	SamplesCY & SamplesCX & corresponding Cluster	& 0\\
\end{array}
\right),
\end{align} where the Samples are sorted by their Clusters.\\
\\
The global variable $stepSize$ is initialized at the beginning, because with it the ploting correctnes can be increased or decreased easily. These changes affect the computing duration very much. Hence, for the test runs, a big step size has been chosen ($stepSize$=1) and in the end, it has been adjusted ($stepSize$=0,3).\\
\\
In general, always when one of the three classification functions will be called, a controle function will be used in the second step like an interface, that handels different, variable names and plotting organization. It is presented in the following line:
\begin{align}
plotClassifier(dataTrainingC, dataTrainingP, classificationName, params);
\end{align}
For the classification function of "k Nearest Neighbours" the $classificationName$ is set to 'classifierKnn', for the classification function of "Parzen Windows" it is set to 'classifierParzen' and for the classification function of "Radial Basis Functions" it is set to 'classifierRbf'.\\
\\
Also the individual parameter $params$ have to be set dependent on the called classification function. For the classification function of "k Nearest Neighbours" it is set to 'classifierKnn', for the classification function of "Parzen Windows" it is set to 'classifierParzen' and for the classification function of "Radial Basis Functions" it is set to 'classifierRbf'.\\
\\
This function calculates dependently on the selected classification function the $classified Grid$, the $testClassData$ and forwards the data to the function $plotClsDist$ that plots and saves them.\\
\\
For this, it has to be stated, that $classifiedGrid$ is built in using the function $classifyGrid(...)$. Here, a n*n matrix will be calculated, that has a range from -1 to 2 with the chosen $stepSize$. It recalculates values of certain squares depending on the chosen classification function in order to visualize the decision boundary in every plot. With this matrix, two matrixes called $testClassData1$ and $testClassData2$ are built in using the same function\\
$separateTestDataIntoClasses(...)$. Here, the values of the $classifiedGrid$ are separated corresponding to their classification and the two variables can be plotted in different colours.\\
\\
There are still certain small functions used by all of the three classification techniques that are just selecting or calculating certain points or single variables. In the following, they are just mentioned:\\
$-  function centroids = kmeans(dataTrainingC, k),\\
 -  function phi = phi(x, rbfMu, rbfSigma),\\
 -  function point = getPoint(data, index).\\
$
\\
The function $kmeans(dataTrainingC,k)$ is also implemented as was asked in the exercise in order to calculate the centroids.
\subsection{k Nearest Neighbors (9.1)}
In commenting in the following function, the classification techniques called "k Nearest Neighbours" will be used:
\begin{align}
plotKnn(dataTrainingC, dataTrainingP, k);
\end{align}
The parameter $dataTrainingC$ and $dataTrainingP$ are the same ones, that are already described in the general part. The parameter $k$ is standing for the number of the neighbours to whom the distance will be measured by using this function.
Since the exercise, this function has to be called three times with the changing parameter $k$=1, 5, 25.\\
\\
The function works as described in the following:\\
\\
A loop goes over all training points and calculates the distance between two given points. For this, the function $distance()$ will be used. The results will be saved in an vektror that is sorted from the lowest to the highest distance. Just the $k$ values will be taken in the observations and summed. The rounded and normalized result will be returned and shows the responding classification.
%function _cls = classifierKnn(dataTrainingC, point, isInit)
%function _distance = distance(p1, p2)
\subsection{Parzen Windows (9.2)}
In commenting in the following function, the classification techniques called "Parzen Windows" will be used:\\
\begin{align}
plotParzen(dataTrainingC, dataTrainingP, \sigma^2);
\end{align}
The parameter $dataTrainingC$ and $dataTrainingP$ are the same ones, that are already described in the general part. The parameter $\sigma^2$ is standing for the variance that specifies the Gaussian function with whom the distance and with this the categorization will be specified.
Since the exercise, this function has to be called three times with the changing parameter $\sigma^2$=0,01, 0,1, 0,5.\\
\\
The function works as described in the following:\\
\\
A loop iterates over all training data points and weights this point's distance to the to be tested point by a Gaussian function (the current data point as $\mu$ and the variable parzenSigma as $\sigma$). The weighted distances are summed up (for each class) and the predicted class is the one having the highest sum.
\\
\subsection{Radial Basis Functions (9.3)}
In commenting in the following function, the classification techniques called "Parzen Windows" will be used:\\
\begin{align}
plotRbf(dataTrainingC, dataTrainingP, k, \sigma);
\end{align}
The parameter $dataTrainingC$ and $dataTrainingP$ are the same ones, that are already described in the general part. The parameter $k$ is standing for the chosen number of clusters and $\sigma$ is standing for the width that specifies the Gaussian function and with this the radial basis function.
Since the exercise, this function has to be called four times with the changing parameter $k$=4, 5 and $\sigma$=0,1, 0,5.\\
\\
The function works as described in the following:\\
\\
At first, the kmeans algorithm (from the script) is used to seperate the training data points into k cluster. Then, the weight vector is calculated as described in exercise notes. We use two weight vectors: One for each class. For it's calulation, the corresponding class's labels are set to 1, the others to zero. To determine a new data point's class, one has to compare the network's output for each weight vector. The highelst output determines the class.


\section{Results}

\newcommand{\classifierPlotWidth}[0]{0.85}

In all plots you can see the data related to $C_1$ colored in red, related to $C_2$ colored in blue.
The corresponding decision boundaries $C_1$ and $C_2$ are colored adequately.


\subsection{k Nearest Neighbors (9.1)}
The the plots for $k = 1, 5, 25$ are presented in fig.
\ref{fig:out_classifierKnn_k_1},
\ref{fig:out_classifierKnn_k_5} and
\ref{fig:out_classifierKnn_k_25}.

\image{out_classifierKnn_k_1}{\classifierPlotWidth}%
	{The training patterns and \emph{$k$ Nearest Neighbors} decision boundaries for $k = 1$.}

\image{out_classifierKnn_k_5}{\classifierPlotWidth}%
	{The training patterns and decision boundaries for $k = 5$.}

\image{out_classifierKnn_k_25}{\classifierPlotWidth}%
	{The training patterns and decision boundaries for $k = 25$.}

At the set with $k = 1$, it becomes possible that single far spread points
create their own bubble in the decission boundary landscape.
We may think of this as overfitting.
The greater the chosen $k$, the higher the probability that these errors are eliminated.
Though, the danger of underfitting increases at the same time.


\subsection{Parzen Windows (9.2)}
The the plots for $\sigma^2 = 0.01, 0.1, 0.5$ are presented in fig.
\ref{fig:out_classifierParzen_sigma2_001},
\ref{fig:out_classifierParzen_sigma2_01} and
\ref{fig:out_classifierParzen_sigma2_05}.

\image{out_classifierParzen_sigma2_001}{\classifierPlotWidth}%
	{The training patterns and \emph{Parzen Windows} decision boundaries for $\sigma = 0.01$.}

\image{out_classifierParzen_sigma2_01}{\classifierPlotWidth}%
	{The training patterns and \emph{Parzen Windows} decision boundaries for $\sigma = 0.1$.}

\image{out_classifierParzen_sigma2_05}{\classifierPlotWidth}%
	{The training patterns and \emph{Parzen Windows} decision boundaries for $\sigma = 0.5$.}

The greater the $\sigma$, the bigger becomes the influence of great distances.
In the pictures we can see this in an expanding bubble size.
Similar to the previous classification algorithm, the danger of overfitting decreases with higher $\sigma$,
but can theoretically also result in underfitting.
This was for us not detecable at all.


\subsection{Radial Basis Functions (9.3)}
The the plots for number of clusters $k = 4$ and different $\sigma$s are presented in fig.
\ref{fig:out_classifierRbf_k_4_sigma_001},
\ref{fig:out_classifierRbf_k_4_sigma_0015},
\ref{fig:out_classifierRbf_k_4_sigma_002},
\ref{fig:out_classifierRbf_k_4_sigma_0025} and
\ref{fig:out_classifierRbf_k_4_sigma_05},
while the ones ofr $k = 8$ can be found in fig.
\ref{fig:out_classifierRbf_k_8_sigma_001},
\ref{fig:out_classifierRbf_k_8_sigma_0015},
\ref{fig:out_classifierRbf_k_8_sigma_002},
\ref{fig:out_classifierRbf_k_8_sigma_0025} and
\ref{fig:out_classifierRbf_k_8_sigma_05}.

\image{out_classifierRbf_k_4_sigma_001}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 4$ and $\sigma = 0.01$.}

\image{out_classifierRbf_k_4_sigma_0015}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 4$ and $\sigma = 0.015$.}

\image{out_classifierRbf_k_4_sigma_002}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 4$ and $\sigma = 0.02$.}

\image{out_classifierRbf_k_4_sigma_0025}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 4$ and $\sigma = 0.025$.}

\image{out_classifierRbf_k_4_sigma_05}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 4$ and $\sigma = 0.5$.}

\image{out_classifierRbf_k_8_sigma_001}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 8$ and $\sigma = 0.01$.}

\image{out_classifierRbf_k_8_sigma_0015}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 8$ and $\sigma = 0.015$.}

\image{out_classifierRbf_k_8_sigma_002}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 8$ and $\sigma = 0.02$.}

\image{out_classifierRbf_k_8_sigma_0025}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 8$ and $\sigma = 0.025$.}

\image{out_classifierRbf_k_8_sigma_05}{\classifierPlotWidth}%
	{The training patterns and \emph{RBF} decision boundaries for $k = 8$ and $\sigma = 0.5$.}

In searching an effective set of params, we noticed that $k$ and $\sigma$ may not be smaller than $k = 4$ and $\sigma = 0.015$.
Other values result in the fact that only one class will be detected,
for example when setting $k = 4$ and $\sigma = 0.01$.
The idea of over- and underfitting out of the previous classification methods can be found here too.

\end{document}
