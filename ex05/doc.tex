\documentclass[a4paper,headings=small]{scrartcl}
\KOMAoptions{DIV=12}

\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}

% define style of numbering
\numberwithin{equation}{section} % use separate numbering per section
\numberwithin{figure}{section}   % use separate numbering per section

\title{Machine Intelligence I - WS2011/2012\\Excercise 5}
\author{Rolf Schroeder (340126), Robin Vobruba (343773)}
\date{23. November 2011}

\pdfinfo{%
  /Title    (TU Berlin - Machine Intelligence I - WS2011/2012 - Excercise 5)
  /Author   (Rolf Schroeder (340126), Robin Vobruba (343773))
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}



\begin{document}


\maketitle

\includegraphics[width=\linewidth]{gradientSpace.png}


\newpage
\section{Algorithms}


\subsection{Gradient Descent}

Gradient (or steepest) descent with constant learning rate.

\includegraphics[width=\linewidth]{gradientDescent_weightsEvolution.png}

\includegraphics[width=0.5\linewidth]{gradientDescent_approximation.png}
\includegraphics[width=0.5\linewidth]{gradientDescent_errorsEvolution.png}

The number of steps this method takes to converge for the given problem,
depending on the initial weight values, ranges from two to four.


\newpage
\subsection{Line Search}

Steepest descent combined with a line search method to determine the learning rate.

\includegraphics[width=\linewidth]{lineSearch_weightsEvolution.png}

\includegraphics[width=0.5\linewidth]{lineSearch_approximation.png}
\includegraphics[width=0.5\linewidth]{lineSearch_errorsEvolution.png}

Usually, this method takes two or three iterations for the given problem,
to reach its optimum (the error does not change anymore).


\newpage
\subsection{Conjugate Gradient}

Conjugate gradient method.

\includegraphics[width=\linewidth]{conjugateGradient_weightsEvolution.png}

\includegraphics[width=0.5\linewidth]{conjugateGradient_approximation.png}
\includegraphics[width=0.5\linewidth]{conjugateGradient_errorsEvolution.png}

This method arrives at the optimal solution within $n$ iterations,
with $n$ being the dimensionality of our problem space,
which here is the number of weights: two.
Other then the two methods above, which both slowly approach the optimal
weight space position, this one will usually enlarge the error by a big deal
in the first step.
It is the most robust of the three algorithms.


\newpage
\section{Evaluation}

\begin{list}{}{}
	\item Gradient Descent $\Rightarrow$ tuedeli due
	\item Line Search $\Rightarrow$ toeroeoeoeoe
	\item Conjugate Gradient $\Rightarrow$ goody goodgood
\end{list}


or more formally:
\begin{list}{}{}
	\item Conjugate Gradient $>$ Line Search $>$ Gradient Descent
\end{list}


\end{document}
